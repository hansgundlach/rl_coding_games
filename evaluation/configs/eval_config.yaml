# MBPP Evaluation Configuration
# This file allows easy configuration of evaluation parameters

# Evaluation Control
enabled: true                    # Enable/disable evaluation entirely
num_questions: 10                # Number of MBPP problems to evaluate (default: 10 for quick eval)
eval_at_start: true              # Run evaluation at start of training
eval_at_end: true                # Run evaluation at end of training
eval_interval_steps: null        # Run evaluation every N steps (null = no intermediate evals)

# Dataset Configuration
dataset_path: null               # Path to MBPP dataset (null = auto-detect)
use_sanitized: true              # Use sanitized MBPP (427 problems) vs full MBPP (974 problems)
test_split: "test"               # Which split to use: "test", "validation", "train"

# Execution Settings
timeout_seconds: 10              # Maximum execution time per problem
max_memory_mb: 128               # Memory limit for code execution
safe_execution: true             # Use safe execution environment

# Model Generation Settings
max_new_tokens: 512              # Maximum tokens to generate
temperature: 0.2                 # Sampling temperature (lower = more deterministic)
do_sample: true                  # Enable sampling

# Output Settings
save_results: true               # Save detailed evaluation results
results_dir: "./eval_results"    # Directory to save results
verbose: true                    # Print detailed evaluation progress

# Environment-specific overrides
# Uncomment and modify as needed for your specific setup

# For MIT Supercloud V100 (offline environment):
# dataset_path: "/home/gridsan/[username]/data/mbpp/sanitized-mbpp.json"
# num_questions: 5               # Smaller for memory constraints
# timeout_seconds: 5             # Shorter timeout
# max_new_tokens: 256            # Shorter generations

# For Lambda A100 (online environment):
# num_questions: 20              # Can handle more problems
# timeout_seconds: 15            # More generous timeout
# max_new_tokens: 768            # Longer generations

# Advanced Configuration Examples:

# Quick development/testing:
# num_questions: 3
# eval_interval_steps: 50
# verbose: true

# Production evaluation:
# num_questions: 50
# eval_interval_steps: null
# save_results: true

# Continuous monitoring during training:
# eval_interval_steps: 100
# num_questions: 5
# verbose: false