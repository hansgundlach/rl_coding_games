# GRPO Code Game with ICL Memory Opponent Configuration
# Generator trained with GRPO, Guesser uses frozen weights + ICL memory

# Random seed configuration for reproducible experiments
seed: 42  # Global seed for all random number generators

# Model Configuration
generator_model:
  id: "Qwen/Qwen3-4B"  # Trainable generator model
  cache_dir: "./model_cache"

guesser_model:
  id: "Qwen/Qwen3-1.7B"  # Frozen guesser model (same family)
  cache_dir: "./model_cache"

# LoRA Configuration for Generator
lora:
  task_type: "CAUSAL_LM"
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# ICL Memory Configuration
icl:
  memory_size: 5               # Max examples to keep in memory
  refresh_every: 5             # Games between memory updates
  snapshot_max: 8               # Max frozen snapshots to keep
  p_latest: 0.6                 # 60% latest ICL, 40% frozen snapshots

# Game Configuration
game:
  timeout: 3                   # Code execution timeout in seconds

# Generation Configuration
generation:
  generator_max_tokens: 512     # Max tokens for code generation
  guesser_max_tokens: 1024       # Max tokens for output prediction
  guesser_temperature: 0.3      # Higher temperature handicap for guesser
  temperature: 0.8
  top_p: 0.9
  top_k: 50
  do_sample: true

# Training Configuration
training:
  num_steps: 1000                 # Training steps
  games_per_step: 8             # Games per training step
  save_interval: 10             # Save checkpoint every N steps
  checkpoint_dir: "./checkpoints/grpo_code_game_icl"

# GRPO Training Arguments
training_args:
  learning_rate: 0.00002
  beta: 0.05
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  max_prompt_length: 512
  max_completion_length: 512
  num_generations: 2
  optim: "adamw_torch"
  num_train_epochs: 1
  gradient_checkpointing: false
  dataloader_pin_memory: true
  remove_unused_columns: false
  logging_steps: 1
  max_steps: 50
  # ------------------------------------------------------------------
  # Additional GRPO hyperparameters (all set to library defaults)
  # Adjust these later to experiment without touching the code.
  epsilon: 0.2               # trust-region lower-bound (PPO-style clipping)
  num_iterations: 1          # μ – number of optimisation mini-steps per batch
  # ------------------------------------------------------------------
  output_dir: "./outputs/grpo_code_game_icl"

# Dataset Configuration
dataset:
  size: 100

# Evaluation Configuration
evaluation:
  enabled: true
  enabled_initial: true
  enabled_final: true
  enabled_interval: true
  eval_interval_steps: 10
  consistent_questions: true  # Use same questions for all interval evaluations (default: true)
  num_questions: 20
  temperature: 0.0
  do_sample: false            # Required when temperature=0.0 for greedy decoding
  max_new_tokens: 512
  timeout_seconds: 10
  results_dir: "./eval_results"
  dataset_path: null  # Auto-detect

# W&B Configuration
wandb:
  enabled: true
  project_name_prefix: "grpo-code-game-icl"  # Project name (no timestamp - all runs go to same project)
  use_consistent_project: true  # Use same project name for all runs
  run_name_format: "grpo-code-game-icl-{timestamp}"  # Format for run names

# vLLM Configuration (optional)
vllm:
  enabled: true
  server:
    host: "127.0.0.1"
    port: 8000
    gpu_memory_utilization: 0.7           # Safer for V100 16GB; raise if stable
    max_model_len: 2048
    tensor_parallel_size: 1               # One V100 per process
    max_num_batched_tokens: 2048          # Keep conservative
    max_num_seqs: 32
    swap_space: 8                         # GB of host swap space for paging KV
    dtype: "float16"                      # No bf16 on V100
    trust_remote_code: true               # Needed for Qwen models
    disable_log_requests: true
    offline_mode: true
  client:
    base_url: "http://127.0.0.1:8000"     # Root; client will add /v1
    timeout: 90
    max_retries: 3
  generation:
    temperature: 0.8
    top_p: 0.9
    max_tokens: 512
    presence_penalty: 0.0
    frequency_penalty: 0.0
    stop: null
  integration:
    use_for_grpo_completions: true
    use_for_evaluation: true
    log_performance_comparison: false
    auto_start_server: true
    wait_for_server: 120                  # Give V100 slow boots time
    fallback_to_hf: true

# Debug Configuration
debug:
  show_detailed_games: 2            # Number of games per step to show detailed results
  max_code_chars: 500               # Maximum characters of code to display
  show_full_responses: true        # Show full model responses (can be very long)
  show_execution_details: true      # Show code execution results and errors