# GRPO Code Game with ICL Memory Opponent Configuration
# Generator trained with GRPO, Guesser uses frozen weights + ICL memory

# Model Configuration
generator_model:
  id: "Qwen/Qwen3-1.7B"  # Trainable generator model
  cache_dir: "./model_cache"

guesser_model:
  id: "Qwen/Qwen2.5-1.5B"  # Frozen guesser model (same family)
  cache_dir: "./model_cache"

# LoRA Configuration for Generator
lora:
  task_type: "CAUSAL_LM"
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# ICL Memory Configuration
icl:
  memory_size: 2                # Max examples to keep in memory
  refresh_every: 12             # Games between memory updates
  snapshot_max: 8               # Max frozen snapshots to keep
  p_latest: 0.5                 # 60% latest ICL, 40% frozen snapshots

# Game Configuration
game:
  timeout: 10                   # Code execution timeout in seconds

# Generation Configuration
generation:
  generator_max_tokens: 512     # Max tokens for code generation
  guesser_max_tokens: 64        # Max tokens for output prediction
  guesser_temperature: 1.0      # Higher temperature handicap for guesser
  temperature: 0.8
  top_p: 0.9
  top_k: 50
  do_sample: true

# Training Configuration
training:
  num_steps: 50                 # Training steps
  games_per_step: 8             # Games per training step
  save_interval: 10             # Save checkpoint every N steps
  checkpoint_dir: "./checkpoints/grpo_code_game_icl"

# GRPO Training Arguments
training_args:
  learning_rate: 0.00002
  beta: 0.1
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  max_prompt_length: 512
  max_completion_length: 512
  num_generations: 2
  optim: "adamw_torch"
  num_train_epochs: 1
  gradient_checkpointing: false
  dataloader_pin_memory: true
  remove_unused_columns: false
  logging_steps: 1
  max_steps: 50
  # ------------------------------------------------------------------
  # Additional GRPO hyperparameters (all set to library defaults)
  # Adjust these later to experiment without touching the code.
  epsilon: 0.2               # trust-region lower-bound (PPO-style clipping)
  epsilon_high: null         # when null, uses same value as `epsilon`
  delta: null                # set a float > 1+epsilon to enable two-sided clipping
  num_iterations: 1          # μ – number of optimisation mini-steps per batch
  loss_type: "bnpo"         # "grpo", "bnpo", or "dr_grpo"
  importance_sampling_level: "token"  # "token" or "sequence"
  scale_rewards: true        # normalise rewards by std-dev (Dr-GRPO suggests false)
  mask_truncated_completions: false    # ignore truncated samples in loss
  sync_ref_model: false
  ref_model_mixup_alpha: 0.6
  ref_model_sync_steps: 512
  top_entropy_quantile: 1.0  # 1.0 keeps all tokens; e.g. 0.2 keeps top-entropy tokens
  # ------------------------------------------------------------------
  output_dir: "./outputs/grpo_code_game_icl"

# Dataset Configuration
dataset:
  size: 100

# Evaluation Configuration
evaluation:
  enabled: true
  enabled_initial: true
  enabled_final: true
  enabled_interval: true
  eval_interval_steps: 10
  num_questions: 5
  temperature: 0.2
  max_new_tokens: 512
  timeout_seconds: 10
  results_dir: "./eval_results"
  dataset_path: null  # Auto-detect

# W&B Configuration
wandb:
  enabled: true
  project_name_prefix: "grpo-code-game-icl"

# vLLM Configuration (optional)
vllm:
  enabled: true
  port: 8000
  host: "127.0.0.1"
  gpu_memory_utilization: 0.85
  max_model_len: 2048
  integration:
    use_for_grpo_completions: false
    log_performance_comparison: false

# Debug Configuration
debug:
  show_detailed_games: 2            # Number of games per step to show detailed results
  max_code_chars: 500               # Maximum characters of code to display
  show_full_responses: false        # Show full model responses (can be very long)
  show_execution_details: true      # Show code execution results and errors