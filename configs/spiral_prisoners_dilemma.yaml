# SPIRAL Iterated Prisoner's Dilemma Configuration
# Self-play training where LLMs compete by submitting code strategies for the prisoner's dilemma

# Model configuration
model:
  id: "Qwen/Qwen3-1.7B"
  cache_dir: "./model_cache"

# LoRA configuration for efficient training
lora:
  task_type: "CAUSAL_LM"
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Game-specific settings
game:
  timeout: 3  # Code execution timeout in seconds per strategy call
  num_rounds: 10  # Number of rounds in each prisoner's dilemma game
  payoff_matrix:
    # Payoff matrix: (my_action, opponent_action): (my_payoff, opponent_payoff)
    cooperate_cooperate: [3, 3]
    cooperate_defect: [0, 5]
    defect_cooperate: [5, 0]
    defect_defect: [1, 1]

# Generation parameters
generation:
  max_new_tokens: 500              # Maximum response length for strategy code
  temperature: 1.0                  # Sampling temperature
  top_p: 1.0                        # Top P sampling (1.0 = disabled)
  top_k: -1                         # Top K sampling (-1 = disabled)
  do_sample: true                   # Enable sampling

# Training parameters
training:
  num_steps: 100
  learning_rate: 0.00002
  rae_alpha: 0.95                   # Role-conditioned advantage estimation decay rate
  gradient_clip_norm: 1.0
  games_per_step_v100: 2            # Conservative for V100 memory constraints
  games_per_step_other: 4           # Standard for A100+ GPUs
  save_interval: 20                 # Save checkpoint every N steps
  checkpoint_dir: "checkpoints/spiral_prisoners_dilemma"

# Evaluation configuration
evaluation:
  enabled: true
  enabled_initial: true       # Run evaluation before training
  enabled_final: true         # Run evaluation after training  
  enabled_interval: true      # Run evaluation during training
  eval_interval_steps: 10     # Run evaluation every N steps
  
  # MBPP evaluation settings (for general coding ability)
  num_questions: 20            # Reduced for faster evaluation during training
  dataset_path: null          # Auto-detect MBPP dataset location
  results_dir: "./eval_results/spiral_prisoners_dilemma"
  temperature: 0.2
  max_new_tokens: 256
  timeout_seconds: 10

# Debug Configuration
debug:
  show_detailed_games: 2            # Number of games per step to show detailed results
  max_code_chars: 500               # Maximum characters of code to display
  show_full_responses: false        # Show full model responses (can be very long)
  show_execution_details: true      # Show code execution results and errors

# Weights & Biases configuration  
wandb:
  enabled: true
  project_name_prefix: "spiral-prisoners-dilemma"

# vLLM configuration (optional, for faster inference)
vllm:
  enabled: false              # Disabled by default
  gpu_memory_utilization: 0.85
  max_model_len: 2048
  trust_remote_code: true
  
  integration:
    use_for_grpo_completions: false
    log_performance_comparison: true