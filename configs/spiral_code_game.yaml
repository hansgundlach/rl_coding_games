# SPIRAL Code Generation Game Configuration
# Self-play training where one player generates code and the other guesses the output

# Model configuration
model:
  id: "Qwen/Qwen3-1.7B"
  cache_dir: "./model_cache"

# Random seed configuration for reproducible experiments
seed: 42  # Global seed for all random number generators

# LoRA configuration for efficient training
lora:
  task_type: "CAUSAL_LM"
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Game-specific settings
game:
  timeout: 3  # Code execution timeout in seconds

# Generation parameters
generation:
  max_new_tokens: 400              # Maximum response length
  temperature: 1.0                  # Sampling temperature
  top_p: 1.0                        # Top P sampling (1.0 = disabled)
  top_k: -1                         # Top K sampling (-1 = disabled)
  do_sample: true                   # Enable sampling
  
  # Role-specific overrides
  generator_max_tokens: 1024        # For code generation
  guesser_max_tokens: 500           # For output prediction (shorter)

# Training parameters
training:
  num_steps: 1000
  
  # Optimizer configuration
  optimizer: "AdamW"                # Changed from Adam to AdamW
  learning_rate: 0.000001           # 1×10^-6 (reduced from 0.00002)
  learning_rate_scheduler: "constant"
  adam_beta1: 0.9                   # β1 parameter for Adam/AdamW
  adam_beta2: 0.95                  # β2 parameter for Adam/AdamW  
  weight_decay: 0.0                 # Weight decay
  
  # Gradient and clipping
  gradient_clip_norm: 1.0           # Already correct
  
  # Batch size and game configuration
  batch_size: 128                   # Target batch size
  games_per_step_v100: 2           # Increased to approach batch_size=128
  games_per_step_other: 4          # Increased for better GPUs
  
  # SPIRAL-specific parameters
  rae_alpha: 0.95                   # EMA decay rate (already correct)
  discount_factor: 1.0              # Discount factor for rewards
  inner_proximal_update_epochs: 2   # Inner update epochs
  
  # Policy optimization parameters (for future PPO-style extensions)
  kl_loss_coefficient: 0.0          # KL divergence loss coefficient
  kl_penalty_coefficient: 0.0       # KL penalty coefficient
  policy_clipping_parameter: 0.2    # Policy clipping parameter
  
  # Checkpointing and logging
  save_interval: 20                 # Save checkpoint every N steps
  checkpoint_dir: "checkpoints/spiral_code_game"

# Debug Configuration
debug:
  show_detailed_games: 2            # Number of games per step to show detailed results
  max_code_chars: 500               # Maximum characters of code to display
  show_full_responses: false        # Show full model responses (can be very long)
  show_execution_details: true      # Show code execution results and errors
  show_all_games: false             # Legacy: show all games (very verbose)

# Evaluation configuration
evaluation:
  enabled: true
  enabled_initial: true       # Run evaluation before training
  enabled_final: true         # Run evaluation after training  
  enabled_interval: true      # Run evaluation during training
  eval_interval_steps: 20     # Run evaluation every N steps
  consistent_questions: true  # Use same questions for all interval evaluations (default: true)
  
  # MBPP evaluation settings
  num_questions: 20            # Reduced for faster evaluation during training
  dataset_path: null          # Auto-detect MBPP dataset location
  results_dir: "./eval_results/spiral_code_game"
  temperature: 0.2
  max_new_tokens: 512
  timeout_seconds: 10

# Weights & Biases configuration  
wandb:
  enabled: true
  project_name_prefix: "spiral-code-game"

# vLLM configuration (optional, for faster inference)
vllm:
  enabled: false              # Disabled by default
  gpu_memory_utilization: 0.85
  max_model_len: 2048
  trust_remote_code: true
  
  integration:
    use_for_grpo_completions: false
    log_performance_comparison: true