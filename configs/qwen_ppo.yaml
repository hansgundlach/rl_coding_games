# Qwen PPO Training Configuration

# Environment settings
environment:
  rows: 6
  columns: 7
  reward_shaping: true

# Qwen model settings
qwen:
  model_name: "Qwen/Qwen2.5-3B"
  device: "auto"  # "cuda", "cpu", or "auto"
  max_new_tokens: 10
  temperature: 0.7

# LoRA configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"

# PPO hyperparameters
ppo:
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01

# Training settings
training:
  num_epochs: 2  # Reduced for testing
  episodes_per_epoch: 5  # Reduced for testing
  batch_size: 2  # Reduced for testing
  learning_rate: 5e-5
  weight_decay: 0.01
  max_grad_norm: 1.0

# Checkpointing
checkpoints:
  save_dir: "checkpoints/qwen_ppo"
  save_interval: 1  # Save every epoch for testing

# Logging
logging:
  project: "connectx-qwen-rl"
  entity: null  # Set your W&B entity if needed
  log_interval: 1

# Evaluation
evaluation:
  skip_initial_eval: true  # Skip initial evaluation for faster startup
  coding_eval_interval: 5  # Evaluate coding performance every N epochs
  num_coding_samples: 1    # Number of samples for coding evaluation