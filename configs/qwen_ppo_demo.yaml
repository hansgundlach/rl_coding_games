# Qwen PPO Training Configuration - Ultra-Fast Demo (< 10 minutes)

# Environment settings
environment:
  rows: 6
  columns: 7
  reward_shaping: true

# Qwen model settings
qwen:
  model_name: "Qwen/Qwen2.5-3B"
  device: "auto"  # "cuda", "cpu", or "auto"
  max_new_tokens: 10
  temperature: 0.7

# LoRA configuration
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"

# PPO hyperparameters
ppo:
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01

# Training settings - Ultra-fast demo
training:
  num_epochs: 3            # Very few epochs for demo
  episodes_per_epoch: 2    # Minimal episodes
  batch_size: 1            # Smallest batch for speed
  learning_rate: 5e-5
  weight_decay: 0.01
  max_grad_norm: 1.0

# Checkpointing
checkpoints:
  save_dir: "checkpoints/qwen_ppo_demo"
  save_interval: 1         # Save every epoch to see evaluations

# Logging
logging:
  project: "connectx-qwen-rl-demo"
  entity: null  # Set your W&B entity if needed
  log_interval: 1

# Evaluation - Ultra-fast settings
evaluation:
  skip_initial_eval: false    # Show initial baseline
  coding_eval_interval: 1     # Evaluate every single epoch!
  num_coding_samples: 1       # Single sample
  quick_eval: true           # Use quick evaluation
  quick_eval_size: 5         # Very small for speed (5 problems each)
  full_eval_interval: 0      # No full evaluation for demo