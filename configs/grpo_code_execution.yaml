# configs/grpo_code_execution.yaml

# --- Model Settings ---
model:
  id: "Qwen/Qwen2.5-1.5B"  # Base model to use (will try to find 1.5B, then 3B in cache)
  cache_dir: "./model_cache"
  # local_files_only will be set by the script based on offline_mode detection

# --- LoRA Configuration ---
lora:
  task_type: "CAUSAL_LM"
  r: 16
  lora_alpha: 32
  target_modules: "all-linear" # Example: "q_proj,v_proj" or "all-linear"

# --- Training Arguments (GRPOConfig) ---
training_args:
  output_dir: "checkpoints/grpo_code_execution"
  learning_rate: 0.00002
  
  # Adaptive Batch/Sequence Lengths (Script will override based on GPU/Model size)
  # For V100 + 3B: batch_size=1, gradient_accumulation_steps=8, max_prompt_length=128, max_completion_length=256
  # For V100 + 1.5B: batch_size=4, gradient_accumulation_steps=2, max_prompt_length=256, max_completion_length=384
  # For A100/H100/Other: batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=512
  per_device_train_batch_size: 8 # Default for other GPUs
  gradient_accumulation_steps: 2 # Default for other GPUs
  max_prompt_length: 512       # Default for other GPUs
  max_completion_length: 512   # Default for other GPUs

  num_generations: 2  # GRPO requires minimum 2
  optim: "adamw_8bit"
  num_train_epochs: 1
  
  # Precision (Script will set bf16/fp16 based on GPU support)
  # bf16: true/false
  # fp16: true/false
  
  gradient_checkpointing: False # Explicitly disabled
  dataloader_pin_memory: True   # Script will set to False for V100

  remove_unused_columns: False
  logging_steps: 1
  max_steps: 100  # Set to 2 for quick pipeline testing

# --- Weights & Biases Logging Settings ---
wandb:
  enabled: True                 # Set to False to completely disable W&B logging
  project_name_prefix: "qwen-code-execution-grpo" # Project name will be: prefix-timestamp

# --- Evaluation Settings (MBPP) ---
evaluation:
  # Evaluation Control
  enabled: True                    # Enable/disable evaluation entirely
  enabled_initial: True            # Run initial MBPP evaluation
  enabled_final: True              # Run final MBPP evaluation
  num_questions: 10                # Number of MBPP problems to evaluate
  eval_interval_steps: 20        # Run evaluation every N steps (null = no intermediate evals)
  
  # Dataset Configuration
  dataset_path: null               # Path to MBPP dataset (null = auto-detect)
  use_sanitized: True              # Use sanitized MBPP (427 problems) vs full MBPP (974 problems)
  test_split: "test"               # Which split to use: "test", "validation", "train"
  
  # Code Execution Settings
  timeout_seconds: 10              # Maximum execution time per problem
  max_memory_mb: 128               # Memory limit for code execution
  safe_execution: True             # Use safe execution environment
  
  # Model Generation Settings (for MBPP evaluation)
  max_new_tokens: 512              # Maximum tokens to generate
  temperature: 0.2                 # Sampling temperature (lower = more deterministic)
  do_sample: True                  # Enable sampling
  
  # Output Settings
  save_results: True               # Save detailed evaluation results
  results_dir: "./eval_results"    # Directory to save results (will be updated to logs dir if in SLURM)
  verbose: True                    # Print detailed evaluation progress

# --- Dataset Configuration ---
dataset:
  size: 1000  # Number of prompts in dataset
  
# --- Code Execution Settings ---
execution:
  timeout: 3  # Timeout in seconds for code execution
  debug_completions: 3  # Number of completions to show detailed debug info for

# --- Reward Function Settings ---
rewards:
  success_with_output: 1.0    # Code runs successfully and produces output
  success_no_output: 0.5      # Code runs successfully but no output  
  syntax_error: -0.5          # Code has syntax/indentation errors
  runtime_error: -1.0         # Code has runtime errors
  timeout_error: -1.0         # Code execution times out

# --- vLLM Integration Settings ---
vllm:
  # Enable vLLM for faster inference (offline-compatible for Supercloud V100s)
  enabled: False               # Set to False to use standard HuggingFace generation
  
  # Server Configuration (V100-optimized for Supercloud)
  server:
    host: "localhost"
    port: 8000
    gpu_memory_utilization: 0.85    # Conservative for V100s (16GB VRAM)
    max_model_len: 2048             # Balanced for V100 memory constraints
    tensor_parallel_size: 1         # Single GPU, increase if multiple V100s available
    max_num_batched_tokens: 4096    # Optimized batch size for V100
    max_num_seqs: 32               # Concurrent sequences
    swap_space: 4                  # GB of CPU swap space
    dtype: "float16"               # Use fp16 for V100 compatibility
    trust_remote_code: True
    
    # Offline mode settings for Supercloud
    offline_mode: True             # Enable offline mode
    disable_log_requests: True     # Reduce logging overhead
    disable_log_stats: False       # Keep performance stats
    
  # Client Configuration
  client:
    base_url: "http://localhost:8000/v1"
    timeout: 60                    # seconds
    max_retries: 3
    
  # Generation Parameters for GRPO Training
  generation:
    grpo_completions:
      max_tokens: 512
      temperature: 0.8
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop: ["```", "\n\n\n"]     # Stop at code blocks or excessive newlines
      
    # Generation Parameters for MBPP Evaluation  
    mbpp_evaluation:
      max_tokens: 512
      temperature: 0.2
      top_p: 0.95
      frequency_penalty: 0.0
      presence_penalty: 0.0
      stop: ["```", "def ", "class ", "\n\n"]
      
  # Integration Control
  integration:
    use_for_grpo_completions: True    # Use vLLM for reward function completions
    use_for_evaluation: True          # Use vLLM for MBPP evaluation
    auto_start_server: True           # Automatically start vLLM server
    wait_for_server: 30              # Seconds to wait for server startup
    fallback_to_hf: True             # Fall back to HuggingFace if vLLM fails
    log_performance_comparison: True  # Log speed comparisons